{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need some cutouts and coordinates\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy import coordinates\n",
    "from astropy import units as u\n",
    "\n",
    "#if you want to try this with various images, glob is the answer for finding files!\n",
    "import glob\n",
    "# filenames = glob.glob('../../Convolved_Images_Hub/*126_image*')\n",
    "# print(filenames)\n",
    "\n",
    "#some common packages\n",
    "import numpy as np\n",
    "from astropy import wcs\n",
    "from astropy.convolution import convolve, Gaussian2DKernel, Box2DKernel\n",
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "from reproject import reproject_exact  #a package that can be added to astropy using anaconda or pip (see their docs pg)\n",
    "from reproject import reproject_interp\n",
    "import sys\n",
    "\n",
    "#easiest option would be using a library...assuming it works...but it doesn't?:(\n",
    "# from statsmodels.distributions.empirical_distribution import ECDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This dictionary defines a colormap in case you want to make a unique colormap?\n",
    "cdict = {'red':  ((0.0, 0.0, 0.0), (0.5, 1.0, 1.0), (1.0, 0.8, 0.8)), 'green': ((0.0, 0.8, 0.8), (0.5, 1.0, 1.0), (1.0, 0.0, 0.0)), 'blue':  ((0.0, 0.0, 0.0), (0.5, 1.0, 1.0), (1.0, 0.0, 0.0)) } # no red at 0\n",
    "# all channels set to 1.0 at 0.5 to create white\n",
    "# set to 0.8 so its not too bright at 1\n",
    "\n",
    "# set to 0.8 so its not too bright at 0\n",
    "# all channels set to 1.0 at 0.5 to create white\n",
    "# no green at 1\n",
    "\n",
    "# no blue at 0\n",
    "# all channels set to 1.0 at 0.5 to create white\n",
    "# no blue at 1\n",
    "\n",
    "\n",
    "# Creating a colormap using the dictionary\n",
    "import matplotlib.colors as colors\n",
    "GnRd = colors.LinearSegmentedColormap('GnRd', cdict)\n",
    "\n",
    "\n",
    "#a plotting code to review what we're analyzing while in python\n",
    "#data is the input data\n",
    "#w is the wcs\n",
    "#wcscond is True or False, True means the axes will be in RA/Dec, and False means axes in pixels\n",
    "import matplotlib.pyplot as plt\n",
    "def implot(data, w, wcscond, vmax_p):\n",
    "    fig = plt.figure()\n",
    "\n",
    "    if  wcscond == True:\n",
    "        fig.add_subplot(111, projection=w)\n",
    "    else:\n",
    "        fig.add_subplot(111)\n",
    "\n",
    "    #for christmas turn on GnRd\n",
    "    #plt.cm.get_cmap('Blues', 6) is another option\n",
    "    #can also use RdBu...\n",
    "    #otherwise just use plt.cm.viridis b/c it's not bad\n",
    "    plt.imshow(data, origin='lower', cmap=plt.cm.viridis, vmin =0, vmax=vmax_p)\n",
    "    plt.xlabel('RA')\n",
    "    plt.ylabel('Dec')\n",
    "\n",
    "\n",
    "#determining cutout coordinates for each knot...so far just do it by eye and in pixel space\n",
    "#could probably do it in wcs coords, RA/DEC, but I couldn't get that working\n",
    "\n",
    "def file_open(file):\n",
    "    hdu1 = fits.open(file)  #import image\n",
    "    w = wcs.WCS(hdu1[0].header)   #get wcs coords\n",
    "    data = hdu1[0].data  #getting data from hdu\n",
    "    hdu1.close()\n",
    "\n",
    "    return w, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "'''now convolve my image with a PSF of the image we're projecting ONTO\n",
    "an approx PSF can be found by assuming a 2D Gaussian func with a width (a FWHM) of the diffrac limit\n",
    "that is the st dev of the Gaussian is about the st dev is about = lambda/D\n",
    "a list of PSFs are found on https://docs.astropy.org/en/stable/convolution/kernels.html\n",
    "\n",
    "Notes:\n",
    "FIRST: always must convert hdu1_pixtorad to radians! It's inconsistent otherwise, and lambda/D is generally in radians\n",
    "\n",
    "what we're using for the gaussian width is the FWHM, not the radius of the first ring of the diffraction pattern,\n",
    "so it's 1.2 not 1.22 times lambda/D\n",
    "\n",
    "D is 85 cm for spitzer\n",
    "D is 2.4 m for hubble\n",
    "'''\n",
    "\n",
    "\n",
    "def im_conv(low_res_name, D, hdu_pix_torad, hdu_dat, kern):\n",
    "    #unfortuantely no good way to find wavelength from header right now. can enter it manually, but I tried to automate it\n",
    "\n",
    "    #reading in excel file of wavelengths...right now needs to be in same directory as this code\n",
    "    #first col is a substring of the fits image file name, the second col is the wavelengths in microns\n",
    "    df = pd.read_excel('../imglams.xlsx')\n",
    "    cols = df.columns\n",
    "    cols_str = [str(i) for i in df[cols[0]]]\n",
    "    #some test cases I was using\n",
    "\n",
    "    #gaussian kernel\n",
    "    if kern == 'gauss':\n",
    "        #this finds the loc in the excel file where the image substring matches our image name\n",
    "        #it then finds the wavelength value corresponding to that loc\n",
    "        lam =  df.loc[np.where([i in low_res_name for i in cols_str])[0][0]].values[1] #lambda in microns\n",
    "\n",
    "        #finding angular resolution...the FWHM of our Gaussian PSF\n",
    "        res = 1.2 * lam / D         #resolution in radians\n",
    "        res = res / hdu_pix_torad        #so converting to pixels\n",
    "\n",
    "        #finding PSF and then calculating the convolution of our image and the PSF of the image we're projecting onto\n",
    "        kernel = Gaussian2DKernel(res)\n",
    "\n",
    "    #box kernel\n",
    "    if kern == 'box':\n",
    "        kernel = Box2DKernel(16.)\n",
    "\n",
    "    hdu_conv = convolve(hdu_dat, kernel)\n",
    "    return hdu_conv\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "#setting up a new fits file to be saved and viewed in DS9\n",
    "#primarily to save the image we reprojected, but can also be used to save the convolved images\n",
    "def fits_saver(array, wcs_header, name, save_path):\n",
    "    '''\n",
    "    array is a 2d array of data - could be from reprojecting one image onto another or from convolution\n",
    "    wcs_header is a header containing the wcs coords of the image that we projected onto or of the orig image (if from the convolution)\n",
    "    name is the path to some image you're using. It will get string split at the / character, and the func only takes the last element of that splitting\n",
    "    save_path is the folder you want to save to...recommended to also add something to the start of the images names to make it clear what you did to them (e.g. 'Regridded/regrid_')\n",
    "    '''\n",
    "\n",
    "    #creating a new file and adding the reprojected array of data as well as the WCS that we projected onto\n",
    "    hdu_new = fits.PrimaryHDU(array, header=wcs_header)\n",
    "    hdul = fits.HDUList([hdu_new])\n",
    "\n",
    "    #saving the file\n",
    "    new_filename = name.split('/')[-1]  #grabs the file name we were using from before\n",
    "    hdul.writeto(save_path+new_filename, overwrite=True)\n",
    "\n",
    "    return (save_path+new_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only saving the fits files\n",
    "def cutout_saver(filenames, pos, size, name, save=False):\n",
    "    for file in filenames:\n",
    "        hdu1 = fits.open(file)  #import image\n",
    "        w = wcs.WCS(hdu1[1].header)   #get wcs coords\n",
    "    #     print(w.array_shape)\n",
    "    #     w = wcs.utils.wcs_to_celestial_frame(w)\n",
    "\n",
    "        #cuting out data and wcs\n",
    "        data = hdu1[1].data\n",
    "        cutout = Cutout2D(data, position, size, wcs = w.celestial)\n",
    "        datacut = cutout.data\n",
    "        wcscut = cutout.wcs\n",
    "    #     print(wcscut.is_celestial)\n",
    "\n",
    "        #updating header with WCS info\n",
    "        newhead = hdu1[0].header.update(wcscut.to_header())\n",
    "        hdu1.close()\n",
    "\n",
    "        #plottinga\n",
    "        implot(datacut, wcscut, False, np.mean(datacut))\n",
    "    #     implot(data, new_wcs)     #plot\n",
    "    #     plt.savefig('datacut.png')\n",
    "    #     sys.exit()\n",
    "\n",
    "\n",
    "        #saving full fits file...\n",
    "        if save == True:\n",
    "            lamnum = file[file.index('build')-3:file.index('build')]\n",
    "            fits.writeto('hh_cutouts/'+name+lamnum+\".fits\", datacut, wcscut.to_header(), overwrite=True)\n",
    "        #     fits.writeto('HH6_'+lamnum+\".fits\", datacut, newhead, overwrite=True)\n",
    "\n",
    "        #     output_hdul = new_wcs.to_fits()\n",
    "        #     output_hdul[0].data = data\n",
    "        #     output_hdul.writeto('HH6_'+file[:3]+\".fits\", overwrite=True)\n",
    "        #     sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the hubble images\n",
    "files_units = ['../../ngc1333_fits/126build_shift_2_drz.fits', '../../ngc1333_fits/164build_shift_2_drz.fits']\n",
    "hdu_list = [fits.open(i) for i in files_units]\n",
    "hdu_header_list = [fits.open(i)[0].header for i in files_units]\n",
    "\n",
    "#IMPORTANT: first load in file to cutout of...\n",
    "# filenames = files_units #glob.glob('../../ngc1333_fits/*drz.fits') #normally glob is convenient, but here we only doing x2\n",
    "# print(filenames)\n",
    "\n",
    "# '''\n",
    "# HH 5: 7095.485, 1391.142\n",
    "# '''\n",
    "# position = (7095.485, 1391.142)  #x, y!\n",
    "# size = (220, 220)  #y, x!...necessary b/c of how cutout works\n",
    "# name = 'hh5_lam'\n",
    "# cutout_saver(files_units, position, size, name, save=True)\n",
    "# '''\n",
    "\n",
    "# '''\n",
    "# # HH 6:\n",
    "# # # '''\n",
    "# position = (4725, 2875)  #x, y!\n",
    "# size = (240, 300)  #y, x!...necessary b/c of how cutout works\n",
    "# name = 'hh6_lam'\n",
    "# cutout_saver(files_units, position, size, name, save=True)\n",
    "# '''\n",
    "# HH 711:\n",
    "# '''\n",
    "# position = (6124.0401,3215.233)  #x, y!\n",
    "# size = (959,1054)  #y, x!...necessary b/c of how cutout works\n",
    "# name = 'hh711_lam'\n",
    "# cutout_saver(files_units, position, size, name, save=True)\n",
    "# '''\n",
    "# HH 13: 10539.368, 3924.647\n",
    "# '''\n",
    "# position = (10539.368, 3924.647)  #x, y!\n",
    "# size = (220, 220)  #y, x!...necessary b/c of how cutout works\n",
    "# name = 'hh13_lam'\n",
    "# cutout_saver(files_units, position, size, name, save=True)\n",
    "# '''\n",
    "# HH 15: 9773.224, 2878.197\n",
    "# '''\n",
    "# position = (9773.224, 2878.197)  #x, y!\n",
    "# size = (250, 250)  #y, x!...necessary b/c of how cutout works\n",
    "# name = 'hh15_lam'\n",
    "# cutout_saver(files_units, position, size, name, save=True)\n",
    "# '''\n",
    "# '''\n",
    "# HH 16:\n",
    "# '''\n",
    "# position = (9081.8719, 2105.2402)  #x, y!\n",
    "# size = (70, 150)  #y, x!...necessary b/c of how cutout works\n",
    "# name = 'hh16_lam'\n",
    "# cutout_saver(files_units, position, size, name, save=True)\n",
    "# # '''\n",
    "# # HH 17: looks like a star\n",
    "# # '''\n",
    "# position = (4512.2483, 2073.0366)  #x, y!\n",
    "# size = (250, 250)  #y, x!...necessary b/c of how cutout works\n",
    "# name = 'hh19_lam'\n",
    "# cutout_saver(files_units, position, size, name, save=True)\n",
    "# # '''\n",
    "# # '''\n",
    "# # HH 18: Cutoff from image:(\n",
    "# # '''\n",
    "# # HH 340:\n",
    "# # '''\n",
    "# position = (11240.533, 3900.8797)  #x, y!\n",
    "# size = (200, 150)  #y, x!...necessary b/c of how cutout works\n",
    "# name = 'hh340_lam'\n",
    "# cutout_saver(files_units, position, size, name, save=True)\n",
    "# # '''\n",
    "# # HH 341: 9335.032, 3892.378\n",
    "# # '''\n",
    "position = (9335.032, 3892.378)  #x, y!\n",
    "size = (250, 250)  #y, x!...necessary b/c of how cutout works\n",
    "name = 'hh341_lam'\n",
    "cutout_saver(files_units, position, size, name, save=True)\n",
    "# # '''\n",
    "# # HH 342: Cutoff by pixel:(\n",
    "# # '''\n",
    "\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "#EX: grabbing all the fits image paths in a directory, so they can be looped through and their data opened\n",
    "#set your path to some directory with images (the images can be in subdirectories)\n",
    "#using ** will grab all files even in subdirectories...WARNING this will take longer\n",
    "\n",
    "#initializing some lists to be used\n",
    "hdu_pix_list = []\n",
    "hdu_pixtorad_list = []\n",
    "hdu_fnu_list = []\n",
    "hdu_flam_list = []\n",
    "hdu_bw_list = []\n",
    "hdu_data_list = []\n",
    "hdu_header_list = []\n",
    "\n",
    " #defining new list for the actual image we're using...\n",
    "im_names_hh5 = ['hh_cutouts/hh5_lam126.fits', 'hh_cutouts/hh5_lam164.fits']\n",
    "im_names_hh6 = ['hh_cutouts/hh6_lam126.fits', 'hh_cutouts/hh6_lam164.fits']\n",
    "im_names_hh711 = ['hh_cutouts/hh711_lam126.fits', 'hh_cutouts/hh711_lam164.fits']\n",
    "im_names_hh13 = ['hh_cutouts/hh13_lam126.fits', 'hh_cutouts/hh13_lam164.fits']\n",
    "im_names_hh15 = ['hh_cutouts/hh15_lam126.fits', 'hh_cutouts/hh15_lam164.fits']\n",
    "im_names_hh16 = ['hh_cutouts/hh16_lam126.fits', 'hh_cutouts/hh16_lam164.fits']\n",
    "im_names_hh340 = ['hh_cutouts/hh340_lam126.fits', 'hh_cutouts/hh340_lam164.fits']\n",
    "im_names_hh341 = ['hh_cutouts/hh341_lam126.fits', 'hh_cutouts/hh341_lam164.fits']\n",
    "im_picks_list = [im_names_hh5, im_names_hh6, im_names_hh711, im_names_hh13, im_names_hh15, im_names_hh16, \n",
    "                     im_names_hh340, im_names_hh341]\n",
    "\n",
    "\n",
    "\n",
    "#looping through chosen HHs and regridding\n",
    "hdu_header1 = hdu_list[1][1].header\n",
    "hdu_header2 = hdu_list[1][1].header\n",
    "w = wcs.WCS(hdu_header2)\n",
    "\n",
    "for name, name2 in im_picks_list:\n",
    "    #reading in files\n",
    "    hdu1 = fits.open(name) #126\n",
    "    hdu_header1 = hdu1[0].header\n",
    "    hdu2 = fits.open(name2) #164\n",
    "    hdu_header2 = hdu2[0].header\n",
    "    w = wcs.WCS(hdu_header2)\n",
    "    \n",
    "    hdu_pix1 = (hdu_list[0][0].header['D001SCAL'])  #D001SCAL is the keyword for Hubble images, in sr\n",
    "    hdu_pixtorad1 = (hdu_pix1 / 206265.)\n",
    "    hdu_flam1 = (hdu_list[0][0].header['PHOTFLAM'])\n",
    "    hdu_bw1 = (hdu_list[0][0].header['PHOTBW'])\n",
    "\n",
    "    hdu_pix2 = (hdu_list[1][0].header['D001SCAL'])  #D001SCAL is the keyword for Hubble images, in sr\n",
    "    hdu_pixtorad2 = (hdu_pix2 / 206265.)\n",
    "    hdu_flam2 = (hdu_list[1][0].header['PHOTFLAM'])\n",
    "    hdu_bw2 = (hdu_list[1][0].header['PHOTBW'])\n",
    "    \n",
    "    #reading in data for general use  and header for wcs\n",
    "    #converting by times by flam * bw from e-/sec...should get units of erg/cm^2/sec as above\n",
    "    hdu1_data = hdu1[0].data * hdu_flam1 * hdu_bw1\n",
    "    hdu2_data = hdu2[0].data * hdu_flam2 * hdu_bw2\n",
    "\n",
    "    #convolving images\n",
    "    D = 2.4 #that of Hubble, in m\n",
    "    D *= 1e6 #converting to microns since x m / 1 m * 1E6 microns gets microns, the unit of our wavelength file\n",
    "    \n",
    "    hdu1_conv = im_conv(name, D, hdu_pixtorad1, hdu1_data, 'gauss')\n",
    "    hdu1_conv = im_conv(name, D, hdu_pixtorad2, hdu1_conv, 'gauss')\n",
    "    hdu2_conv = im_conv(name2, D, hdu_pixtorad2, hdu2_data, 'gauss')\n",
    "    hdu2_conv = im_conv(name2, D, hdu_pixtorad1, hdu2_conv, 'gauss')\n",
    "\n",
    "    #converting the convolved image to correct units and saving it so we can reproject it\n",
    "    #conversion needed for hubble case since units are not in terms of surface brightness\n",
    "#     hdu1_conv_scaled = hdu1_conv   #dividing out sr, D001SCAL is key for pixel size in arcsec\n",
    "    hdu2_conv_scaled = hdu2_conv  / hdu_pixtorad2**2. #dividing out sr, D001SCAL is key for pixel size in arcsec\n",
    "\n",
    "    #you'll need to set the WCS to be that of the header you're basing this off of...ie the header\n",
    "    file_start = '../Convolved_Images_Hub/conv_'\n",
    "    conv1_path = fits_saver(hdu1_conv, hdu_header1, name, file_start)\n",
    "    conv2_path = fits_saver(hdu2_conv_scaled, hdu_header2, name2, file_start)\n",
    "\n",
    "\n",
    "    #reprojection of one hdu using the header (coords and pixels) of another\n",
    "    #The first input is the path to the file we're reprojecting. The second input is the header of the image we're projecting ONTO\n",
    "    #para is False for large images (like these hubble ones)\n",
    "    #output is array (a 2D array of data) and footprint (the footprint from the analysis)\n",
    "    para = False\n",
    "    array, footprint = reproject_exact(conv2_path, w, shape_out=hdu1_conv.shape, parallel=para)\n",
    "\n",
    "    file_start = '../Regridded_Hub/regrid_'\n",
    "    regrid_2_path = fits_saver(array * hdu_pixtorad1**2., hdu_header1, name2, file_start)\n",
    "    regrid_2_foot_path = fits_saver(footprint * hdu_pixtorad1**2., hdu_header1, name2, file_start+'footprint_')\n",
    "\n",
    "\n",
    "    # In[21]:\n",
    "    #declaring noise: measured from the RMS of a region of empty sky while opening the image in DS9\n",
    "    #these are newly taken from the HH 6 images...\n",
    "    noise_126 = 0.0465246*1e-16\n",
    "    noise_164 = 0.0561296*1e-16\n",
    "\n",
    "    #now that we have a reprojected hubble image for hdu2 and both are convolved, need to\n",
    "    #divide one by the other...then can use the same wcs header that we projected onto (hdu1's)!\n",
    "\n",
    "    #the signal to noise cutoff? can try with and without\n",
    "    flux01 = hdu1_conv\n",
    "    flux02 = array * hdu_pixtorad1**2.\n",
    "\n",
    "    perc = 0.2 #this tends to work?? Maybe can change...\n",
    "    flux01[flux01/noise_126 < perc] = np.nan\n",
    "    flux02[flux02/noise_164 < perc] = np.nan\n",
    "\n",
    "    #saving just each separate image with the noise filter on\n",
    "    save_path = 'hh_filtered/'\n",
    "    fits_saver(flux01, hdu_header1, name.split('/')[1].split('_')[0]+'_126_ston_'+str(perc)+'.fits', save_path)\n",
    "    fits_saver(flux02, hdu_header1, name.split('/')[1].split('_')[0]+'_164_ston_'+str(perc)+'.fits', save_path)\n",
    "\n",
    "    # remember to have the right header with the wcs below and that it matches the one we're projecting ONTO\n",
    "    data_ratio = np.divide(flux01, flux02, out=np.zeros_like(flux02), where=flux02!=0.) #need to do np.divide to guarantee we get no divide by zero issue...\n",
    "    save_path = 'hh_div_moreratios/regrid_'  #See fits_saver's \"save_path\" description for explanation\n",
    "    fits_saver(data_ratio, hdu_header1, name.split('/')[1].split('_')[0]+'_ston_'+str(perc)+'.fits', save_path)  #saving the reprojected image\n",
    "\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "\n",
    "#specifically for HH 7_11\n",
    "hh711_path = '../../ngc1333_fits/Background_corr/'\n",
    "hdu1_hh711 = fits.open(hh711_path + 'background_corr_' + '126' + '_aligned.fits')\n",
    "hdu2_hh711 = fits.open(hh711_path + 'background_corr_' + '164' + '_aligned.fits')\n",
    "\n",
    "#reading in conversions\n",
    "hdu_pix1 = (hdu_list[0][0].header['D001SCAL'])  #D001SCAL is the keyword for Hubble images, in sr\n",
    "hdu_pixtorad1 = (hdu_pix1 / 206265.)\n",
    "\n",
    "hdu_pix2 = (hdu_list[1][0].header['D001SCAL'])  #D001SCAL is the keyword for Hubble images, in sr\n",
    "hdu_pixtorad2 = (hdu_pix2 / 206265.)\n",
    "\n",
    "#reading in data for general use  and header for wcs, background corrected\n",
    "hdu1_data = hdu1_hh711[0].data\n",
    "hdu2_data = hdu2_hh711[0].data\n",
    "#converting by times by flam * bw from e-/sec...should get units of erg/cm^2/sec as above\n",
    "\n",
    "\n",
    "#first convolve (already regridded)\n",
    "D = 2.4 #that of Hubble, in m\n",
    "D *= 1e6 #converting to microns since x m / 1 m * 1E6 microns gets microns, the unit of our wavelength file\n",
    "\n",
    "hdu1_conv = im_conv('background_corr_' + '126' + '_aligned.fits', D, hdu_pixtorad1, hdu1_data, 'gauss')\n",
    "hdu1_conv = im_conv('background_corr_' + '126' + '_aligned.fits', D, hdu_pixtorad2, hdu1_conv, 'gauss')\n",
    "hdu2_conv = im_conv('background_corr_' + '164' + '_aligned.fits', D, hdu_pixtorad2, hdu2_data, 'gauss')\n",
    "hdu2_conv = im_conv('background_corr_' + '164' + '_aligned.fits', D, hdu_pixtorad1, hdu2_conv, 'gauss')\n",
    "\n",
    "#now divide and filter...\n",
    "perc = 0.8 #this tends to work?? Maybe can change...\n",
    "noise_126 = 2.6e-18\n",
    "noise_164 = 5.766e-18\n",
    "# hdu1_conv[hdu1_conv/noise_126 < perc] = np.nan\n",
    "# hdu2_conv[hdu2_conv/noise_164 < perc] = np.nan\n",
    "hdu1_conv[hdu1_conv <= 5e-18] = np.nan\n",
    "hdu2_conv[hdu2_conv <= 5e-18] = np.nan\n",
    "\n",
    "# remember to have the right header with the wcs below and that it matches the one we're projecting ONTO\n",
    "hdu_header1 = hdu1_hh711[0].header\n",
    "\n",
    "#saving the noise filtered images for hh 7-11\n",
    "save_path = 'hh_div_moreratios/'\n",
    "fits_saver(hdu1_conv, hdu_header1, name.split('/')[1].split('_')[0]+'_126_ston_'+str(perc)+'.fits', save_path)\n",
    "fits_saver(hdu2_conv, hdu_header1, name.split('/')[1].split('_')[0]+'_164_ston_'+str(perc)+'.fits', save_path)\n",
    "\n",
    "#saving the divided image\n",
    "data_ratio = np.divide(hdu1_conv, hdu2_conv, out=np.zeros_like(hdu2_conv), where=hdu2_conv!=0.) #need to do np.divide to guarantee we get no divide by zero issue...\n",
    "# save_path = 'hh_div/conv_'  #See fits_saver's \"save_path\" description for explanation\n",
    "# fits_saver(data_ratio, hdu_header1, 'hh711_'+'_ston_'+str(perc)+'.fits', save_path)  #saving the reprojected image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I just make a separate file with images of the 1.26 to 1.64 ratio\n",
    "# file = '../../Convolved_Images_Hub/conv_126_image.fits'\n",
    "# file = 'regrid_hub_dash_noleakage_ston_0.8.fits'  #this was the one I used before...\n",
    "path = 'southernoutflows_amanda/Frame3-selected/'\n",
    "# file = path+'regrid_lamflam_126_dash_filled_final.fits' # AB: either 126_ or 164_\n",
    "filelist = [path+'regrid_lamflam_126_dash_filled_final.fits', \n",
    "            path+'regrid_lamflam_128_dash_filled_final.fits',\n",
    "            path+'regrid_lamflam_164_dash_filled_final.fits']\n",
    "\n",
    "\n",
    "# for file in filelist:\n",
    "#     hdu1 = fits.open(file)        #import image\n",
    "#     w = wcs.WCS(hdu1[0].header)   #get wcs coords\n",
    "#     data = hdu1[0].data           #get data from hdu\n",
    "#     #hdu1.close()                  #closing just in case? # AB: moved the location of the close\n",
    "\n",
    "#     #plotting to review what data looks like, need to send wcs regardless\n",
    "#     #probably need to make wcs an optional param...\n",
    "#     implot(data, w, False, 0.3)  # AB: change the last number value so plot was easier to see\n",
    "\n",
    "#     #initial guesses for cutout coords of scattered(?) light\n",
    "#     #guessing \n",
    "#     # AB: updated to values for lower region\n",
    "#     coords_list = [[(1995-6, 1460+1), (100, 150)],\n",
    "#                   [(2120-6, 1371+1), (100, 100)],\n",
    "#                   [(1200-6, 2000+1), (150, 300)], \n",
    "#                   [(1480-6, 1950+1), (100,120)],\n",
    "#                   [(2010-6, 1920+1), (180, 200)],\n",
    "#                   [(2480-6, 1960+1), (200, 200)],\n",
    "#                   ]\n",
    "\n",
    "#     #plotting cutouts, probably better loops for this\n",
    "#     for i in range(len(coords_list)):\n",
    "#         #unpacking coords\n",
    "#         position = coords_list[i][0]\n",
    "#         size = coords_list[i][1]\n",
    "\n",
    "#         #cutting out coordinates using Cutout2D\n",
    "#         cutout = Cutout2D(data, position, size, wcs = w)\n",
    "#         datacut = cutout.data\n",
    "#         wcscut = cutout.wcs\n",
    "\n",
    "#         #plotting *cutout*\n",
    "#         implot(datacut, wcscut, False, 0.3) \n",
    "#         #plt.savefig('knot'+str(i+1)) #+6 b/c of indexing, would have to adjust that and path\n",
    "\n",
    "#         # AB: add on to try and save the cutouts as a fits file to do the division\n",
    "#         hdu1[0].data = datacut\n",
    "#         hdu1[0].header.update(wcscut.to_header())\n",
    "#         hdu1[0].writeto('hh_cutouts/amanda_'+file.split('_')[-4]+'_knot'+str(i+1)+'_shift.fits', overwrite=True)\n",
    "\n",
    "#     hdu1.close()\n",
    "\n",
    "\n",
    "# creating the 126/164 ratio for each of the cutouts and saving that value to fits file\n",
    "# written mostly on my own, probaly missing some stuff\n",
    "\n",
    "noise_126 = 2.64e-18\n",
    "noise_128 = 3e-18 # 0.0290156*1e-16\n",
    "noise_164 = 3.335e-18\n",
    "\n",
    "im126 = fits.open(path+'regrid_lamflam_126_dash_filled_final.fits')\n",
    "# hdu_126_flam = im126[0].header['PHOTFLAM']\n",
    "# hdu_126_bw = im126[0].header['PHOTBW']\n",
    "\n",
    "im128 = fits.open(path+'regrid_lamflam_128_dash_filled_final.fits')\n",
    "\n",
    "im164 = fits.open(path+'regrid_lamflam_164_dash_filled_final.fits')\n",
    "# hdu_164_flam = im164[0].header['PHOTFLAM']\n",
    "# hdu_164_bw = im164[0].header['PHOTBW']\n",
    "\n",
    "coords_list =  [\n",
    "                [[(30, 72), (20, 30)], [(105, 42), (50, 50)]],\n",
    "                [[(40, 66), (20, 20)], [(61, 30), (22, 45)]],\n",
    "                [[(118, 91), (48, 55)], [(232, 50), (48, 39)]],\n",
    "                [[(64, 55), (33, 49)]],\n",
    "                [[(27, 21), (19, 19)], [(83, 70), (46, 65)],[(118, 85), (21, 91)], [(148, 120), (33, 56)]],\n",
    "                [[(78, 161), (49, 51)], [(100, 95), (43, 72)], [(139, 50), (44, 25)]]\n",
    "                    ]\n",
    "\n",
    "count = 0\n",
    "knots = [1,2,3,4,5,6]\n",
    "for i in range(len(knots)):\n",
    "    im_126cut = 'hh_cutouts/amanda_126_knot'+str(knots[i])+'_shift.fits'\n",
    "    im_128cut = 'hh_cutouts/amanda_128_knot'+str(knots[i])+'_shift.fits'\n",
    "    im_164cut = 'hh_cutouts/amanda_164_knot'+str(knots[i])+'_shift.fits'\n",
    "\n",
    "    hdu_126 = fits.open(im_126cut)\n",
    "    hdu_126_data = hdu_126[0].data\n",
    "    #hdu_126_data = hdu_126[1].data * hdu_126_flam * hdu_126_bw\n",
    "    wcs_126 = wcs.WCS(hdu_126[0].header)\n",
    "    #noise_126 = noise_126 * hdu_126_flam * hdu_126_bw\n",
    "    \n",
    "    hdu_128 = fits.open(im_128cut)\n",
    "    hdu_128_data = hdu_128[0].data\n",
    "    #hdu_128_data = hdu_128[1].data * hdu_128_flam * hdu_128_bw\n",
    "    wcs_128 = wcs.WCS(hdu_128[0].header)\n",
    "    #noise_128 = noise_128 * hdu_128_flam * hdu_128_bw\n",
    "\n",
    "    hdu_164 = fits.open(im_164cut)\n",
    "    hdu_164_data = hdu_164[0].data\n",
    "    #hdu_164_data = hdu_164[1].data * hdu_164_flam * hdu_164_bw\n",
    "    #noise_164 = noise_164 * hdu_164_flam * hdu_164_bw\n",
    "    \n",
    "    perc = 0.1 #this tends to work?? Maybe can change...\n",
    "    hdu_126_data[hdu_126_data < noise_126*perc] = np.nan  \n",
    "    hdu_128_data[hdu_128_data < noise_128*perc] = np.nan  \n",
    "    hdu_164_data[hdu_164_data < noise_164*perc] = np.nan\n",
    "\n",
    "    data_ratio = np.divide(hdu_126_data, hdu_164_data, out=np.zeros_like(hdu_164_data), where=hdu_164_data!=0.)\n",
    "    #plt.imshow(data_ratio, vmin = 0, vmax = 1)\n",
    "    implot(data_ratio, wcs_126, wcscond=False, vmax_p=2) \n",
    "\n",
    "    hdu_126[0].data = data_ratio\n",
    "    #hdu_126[0].header.update(wcscut.to_header())\n",
    "    hdu_126[0].writeto('hh_div_moreratios/amanda_126to164ratio_knot'+str(i+1)+'.fits', overwrite=True)\n",
    "    placeholder = np.savetxt('hh_div_moreratios/amanda_126to164ratio_knot'+str(i+1)+'.txt', data_ratio)\n",
    "    \n",
    "    data_ratio_speed = np.divide(hdu_164_data, hdu_128_data, out=np.zeros_like(hdu_128_data), where=hdu_128_data!=0.)\n",
    "    #plt.imshow(data_ratio, vmin = 0, vmax = 1)\n",
    "    implot(data_ratio, wcs_126, wcscond=False, vmax_p=2) \n",
    "\n",
    "    hdu_126[0].data = data_ratio_speed\n",
    "    #hdu_126[0].header.update(wcscut.to_header())\n",
    "    hdu_126[0].writeto('hh_div_moreratios/amanda_164to128ratio_knot'+str(i+1)+'.fits', overwrite=True)\n",
    "    placeholder = np.savetxt('hh_div_moreratios/amanda_164to128ratio_knot'+str(i+1)+'.txt', data_ratio_speed)\n",
    "\n",
    "    \n",
    "\n",
    "    #plotting test cutouts...\n",
    "    for j in coords_list[count]:        \n",
    "        #unpacking coords\n",
    "        position = j[0]\n",
    "        size = j[1]\n",
    "\n",
    "        #cutting out coordinates\n",
    "        cutout = Cutout2D(data_ratio, position, size, wcs = wcs_126)\n",
    "        datacut = cutout.data\n",
    "        wcscut = cutout.wcs\n",
    "        \n",
    "        #plotting\n",
    "        implot(datacut, wcs_126, wcscond=False, vmax_p=2) \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in data\n",
    "# file = '1.26_to_1.64_hub_dash_noleakage_ston_0.8.fits'  #reading in file\n",
    "# file = 'HH7_11/nonlintest_hub_dash_noleakage_ston_-1.fits'\n",
    "# file = 'hh_div/conv_'+'background_corr_hh711_'+'_ston_'+str(perc)+'.fits'\n",
    "file = 'hh_div/regrid_hh711_ston_'+str(0.2)+'.fits'\n",
    "w, data = file_open(file)\n",
    "\n",
    "#plotting\n",
    "implot(data, w, False, 3)\n",
    "\n",
    "#coords from above...\n",
    "# coords_list = [[(550, 170), (100, 200)], \n",
    "#                [(600, 340), (120, 70)], \n",
    "#                [(460, 395), (60, 60)], \n",
    "#                [(540, 480), (90, 140)], \n",
    "#                [(530, 570), (90, 90)]\n",
    "#                 ]\n",
    "\n",
    "# coords_list = [[(550, 170), (100, 200)], \n",
    "#                [(586, 357), (80, 60)], \n",
    "#                [(467, 395), (40, 40)], \n",
    "#                [(548, 467), (100, 200)], \n",
    "#                [(539, 565), (90, 90)]\n",
    "#                 ]\n",
    "coords_list = [#[(550, 170), (100, 200)], \n",
    "               [(618, 345), (48, 45)], \n",
    "               [(500, 365), (40, 40)], #[(467, 395), (40, 40)], \n",
    "               [(583, 441), (36, 39)], \n",
    "               [(550, 528), (37,37)]\n",
    "                ]\n",
    "\n",
    "for i in range(len(coords_list)):\n",
    "    #unpacking coords\n",
    "    position = coords_list[i][0]\n",
    "    size = coords_list[i][1]\n",
    "\n",
    "    #cutting out coordinates\n",
    "    cutout = Cutout2D(data, position, size, wcs = w)\n",
    "    datacut = cutout.data\n",
    "    wcscut = cutout.wcs\n",
    "    \n",
    "    #plotting\n",
    "    implot(datacut, w, False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = 'HH6/regrid_hub_longexp_1.26_to_1.64_0.0_ston_0.8.fits'\n",
    "file = 'hh_div/regrid_hh6_ston_0.2.fits'\n",
    "w, data = file_open(file)\n",
    "\n",
    "#plotting\n",
    "implot(data, w, False, 3)\n",
    "\n",
    "coords_list = [[(53, 64), (38, 35)], [(95, 72), (17, 29)],[(103, 116), (35, 34)],\n",
    "               #[(120, 167), (47, 47)],\n",
    "               [(166, 111), (38, 38)],[(175, 163), (30, 29)],[(223, 165), (34, 37)],]\n",
    "\n",
    "hh6_concat = [] #initializing list to join all the hh6 knots\n",
    "\n",
    "for i in range(len(coords_list)):\n",
    "    #unpacking coords\n",
    "    position = coords_list[i][0]\n",
    "    size = coords_list[i][1]\n",
    "\n",
    "    #cutting out coordinates\n",
    "    cutout = Cutout2D(data, position, size, wcs = w)\n",
    "    datacut = cutout.data\n",
    "    wcscut = cutout.wcs\n",
    "    \n",
    "    #plotting\n",
    "    implot(datacut, w, False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = glob.glob('ngc1333/hh_div/*')\n",
    "filenames = glob.glob('hh_div/regrid*'+str(perc)+'*.fits')\n",
    "filenames.remove('hh_div\\\\regrid_hh6_ston_'+str(perc)+'.fits')\n",
    "filenames.remove('hh_div\\\\regrid_hh711_ston_'+str(perc)+'.fits')\n",
    "# file = 'regrid_hub_dash_noleakage_ston_0.8.fits'\n",
    "\n",
    "# hh13, hh15 , hh341 , hh5\n",
    "coords_list =  [\n",
    "              [[(90, 90), (100, 40)], [(125, 75), (70, 30)]],\n",
    "              [[(100, 100), (125, 170)]],\n",
    "                    [[]],\n",
    "                [[]],\n",
    "              [[(150, 150), (100, 50)]],\n",
    "                [[(105, 95), (80, 80)], [(80, 155), (80, 50)]]\n",
    "                    ]\n",
    "\n",
    "count = 0\n",
    "for file in filenames:\n",
    "    w, data = file_open(file)\n",
    "    #plotting\n",
    "    implot(data, w, False, 2)   \n",
    "    plt.title(file)\n",
    "    \n",
    "    #flattening and sorting data\n",
    "    if file == 'hh_div\\\\regrid_hh16_ston_0.2.fits' or file == 'hh_div\\\\regrid_hh340_ston_0.2.fits':\n",
    "        datacut = data\n",
    "        \n",
    "        print(0) \n",
    "    else: \n",
    "        for i in coords_list[count]:        \n",
    "            #unpacking coords\n",
    "            position = i[0]\n",
    "            size = i[1]\n",
    "\n",
    "            #cutting out coordinates\n",
    "            cutout = Cutout2D(data, position, size, wcs = w)\n",
    "            datacut = cutout.data\n",
    "            wcscut = cutout.wcs\n",
    "\n",
    "            #plotting\n",
    "            implot(datacut, w, False, 2) \n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to set up figures...\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(15,15), sharex=True, constrained_layout=True)\n",
    "# fig.suptitle('Fe[II] Line Ratios of Herbig-Haro Objects')\n",
    "\n",
    "\n",
    "#plot formatting\n",
    "#from https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 25\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "ax1.tick_params(axis='both', which='major', width=3)\n",
    "ax1.tick_params(axis='both', which='minor', width=2)\n",
    "ax2.tick_params(axis='both', which='major', width=3)\n",
    "ax2.tick_params(axis='both', which='minor', width=2)\n",
    "\n",
    "\n",
    "from cycler import cycler\n",
    "plt.rc('axes', prop_cycle=(cycler('color', ['green', 'black', 'gray', 'blue', 'brown', 'chocolate', 'salmon',\n",
    "                                           'darkorange', 'gold', 'khaki', 'teal', 'powderblue', 'steelblue',\n",
    "                                           'darkviolet', 'plum', 'magenta', 'deeppink', 'pink'])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# file = 'HH6/regrid_hub_longexp_1.26_to_1.64_0.0_ston_0.8.fits'\n",
    "perc=0.2\n",
    "file = 'hh_div/regrid_hh6_ston_'+str(perc)+'.fits'\n",
    "w, data = file_open(file)\n",
    "\n",
    "# coords_list = [[(50., 65.), (40, 40)], [(97, 75), (40, 40)],[(100, 119), (47, 47)],[(120, 167), (47, 47)],[(167, 115), (44, 44)],[(175, 163), (40, 40)],[(220, 167), (52, 52)],]\n",
    "coords_list = [[(53, 64), (38, 35)], \n",
    "               [(95, 72), (17, 29)],\n",
    "               [(103, 116), (35, 34)],\n",
    "               [(166, 111), (38, 38)],\n",
    "               [(175, 163), (30, 29)],\n",
    "               [(223, 165), (34, 37)],]\n",
    "\n",
    "hh6_concat = [] #initializing list to join all the hh6 knots\n",
    "\n",
    "for i in range(len(coords_list)):\n",
    "    #unpacking coords\n",
    "    position = coords_list[i][0]\n",
    "    size = coords_list[i][1]\n",
    "\n",
    "    #cutting out coordinates\n",
    "    cutout = Cutout2D(data, position, size, wcs = w)\n",
    "    datacut = cutout.data\n",
    "    wcscut = cutout.wcs\n",
    "\n",
    "\n",
    "    #     flatting data and plotting EDF\n",
    "    flat_data = np.sort(datacut.flatten())\n",
    "    \n",
    "    #incase needing to restrict data...\n",
    "    a0 = int(1.4e3)\n",
    "    af = int(1e6)\n",
    "    hh6_concat = np.concatenate((hh6_concat, flat_data), axis=None)\n",
    "\n",
    "xedf, yedf = edf_calc(hh6_concat)\n",
    "# plt.figure(111)\n",
    "plot_color = ax1.plot(xedf, yedf, label =  'HH 6', linewidth = 3.5, linestyle='-')\n",
    "# print(plot_color.get_facecolor())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#reading in data\n",
    "# file = 'HH7_11/1.26_to_1.64_hub_dash_noleakage_ston_0.8.fits'  #reading in file\n",
    "# file = 'HH7_11/nonlintest_hub_dash_noleakage_ston_-1.fits'\n",
    "# file = 'hh_div/conv_'+'background_corr_hh711_'+'_ston_'+str(perc)+'.fits'\n",
    "perc = 0.2\n",
    "file = 'hh_div/regrid_hh711_ston_'+str(perc)+'.fits'\n",
    "w, data = file_open(file)\n",
    "\n",
    "\n",
    "#coords from above...\n",
    "# coords_list = [#[(550, 170), (100, 200)], \n",
    "#                [(586, 357), (80, 60)], \n",
    "#                [(467, 395), (40, 40)], \n",
    "#                [(590, 450), (40, 40)], \n",
    "#                [(550, 525), (40,40)]\n",
    "#                 ]\n",
    "coords_list = [#[(550, 170), (100, 200)], \n",
    "               [(618, 345), (48, 45)], \n",
    "               [(500, 365), (40, 40)], #[(467, 395), (40, 40)], \n",
    "               [(583, 441), (36, 39)], \n",
    "               [(550, 528), (37,37)]\n",
    "                ]\n",
    "\n",
    "hh711_plotcolors = []\n",
    "for i in range(len(coords_list)):\n",
    "    #unpacking coords\n",
    "    position = coords_list[i][0]\n",
    "    size = coords_list[i][1]\n",
    "\n",
    "    #cutting out coordinates\n",
    "    cutout = Cutout2D(data, position, size, wcs = w)\n",
    "    datacut = cutout.data\n",
    "    wcscut = cutout.wcs\n",
    "\n",
    "    #     flattening data\n",
    "    flat_data = datacut.flatten()\n",
    "    xedf, yedf = edf_calc(flat_data)\n",
    "\n",
    "    #restricting data range\n",
    "    idx = (xedf>0.1)*(xedf<3.5)  #returning mask for array indexing\n",
    "    yedf = yedf[idx]\n",
    "    xedf = xedf[idx]\n",
    "\n",
    "    #plotting only a limited range of data to check\n",
    "#     plt.figure(111)\n",
    "    #     plt.scatter(xedf[a0:af], yedf[a0:af], label =  str(i+7), s = 0.1) \n",
    "    if i+8 == 10:\n",
    "        l_size = 3.5\n",
    "        l_style = '-'\n",
    "    else:\n",
    "        l_size = 1\n",
    "        l_style = '--'\n",
    "    \n",
    "    plot_color = ax1.plot(xedf, yedf, label =  'HH '+str(i+8), linewidth=l_size, linestyle=l_style)\n",
    "\n",
    "    \n",
    "'''\n",
    "# coords_list =  [[(563.5, 629.5), (39, 51)],  [(504.5, 578.5), (61, 53)]] #two boxes near hh 11\n",
    "coords_list = [[(849.99, 356.8), (434.4, 700.4)]] #noise box to right side of 7-11\n",
    "# coords_list = [[(431.8, 359.2), (228, 68.4)]] #1 box to the side of hh 9\n",
    "# coords_list = [[(680.2, 548.2), (171.6, 121.2)]] #1 box above hh 8, to left of hh 10/11\n",
    "\n",
    "flat_data = []\n",
    "scatt_light_concat = []\n",
    "\n",
    "for i in range(len(coords_list)):\n",
    "    #unpacking coords\n",
    "    position = coords_list[i][0]\n",
    "    size = coords_list[i][1]\n",
    "\n",
    "    #cutting out coordinates\n",
    "    cutout = Cutout2D(data, position, size, wcs = w)\n",
    "    datacut = cutout.data\n",
    "    wcscut = cutout.wcs\n",
    "\n",
    "\n",
    "    #     flatting data and plotting EDF\n",
    "    flat_data = np.sort(datacut.flatten())\n",
    "    \n",
    "    #incase needing to restrict data...\n",
    "    scatt_light_concat = np.concatenate((scatt_light_concat, flat_data), axis=None)\n",
    "\n",
    "implot(datacut, wcscut, False, 1) \n",
    "\n",
    "xedf, yedf = edf_calc(scatt_light_concat)\n",
    "# plt.figure(111)\n",
    "plot_color = ax1.plot(xedf, yedf, label =  'HH 11 Scattered Light', linewidth = 1, linestyle='-')\n",
    "# print(plot_color.get_facecolor())\n",
    "'''\n",
    "\n",
    "\n",
    "# filenames = glob.glob('ngc1333/hh_div/*')\n",
    "filenames = glob.glob('hh_div/regrid*'+str(perc)+'*.fits')\n",
    "filenames.remove('hh_div\\\\regrid_hh6_ston_'+str(perc)+'.fits')\n",
    "filenames.remove('hh_div\\\\regrid_hh711_ston_'+str(perc)+'.fits')\n",
    "# file = 'regrid_hub_dash_noleakage_ston_0.8.fits'\n",
    "\n",
    "# hh13, hh15 , hh16, hh340, hh341 , hh5\n",
    "coords_list =  [\n",
    "              [[(90, 90), (100, 40)], [(125, 75), (70, 30)]],\n",
    "              [[(100, 100), (125, 170)]],\n",
    "                    [[]],\n",
    "                [[]],\n",
    "              [[(150, 150), (100, 50)]],\n",
    "                [[(105, 95), (80, 80)], [(80, 155), (80, 50)]]\n",
    "                    ]\n",
    "\n",
    "count = 0\n",
    "for file in filenames:    \n",
    "    w, data = file_open(file)  \n",
    "    #plotting\n",
    "#     implot(data, w, False, 3) \n",
    "    \n",
    "    hh_concat = [] #initializing list to join all the hh6 knots\n",
    "    \n",
    "    #flattening and sorting data\n",
    "    if file == 'hh_div\\\\regrid_hh16_ston_0.2.fits' or file == 'hh_div\\\\regrid_hh340_ston_0.2.fits':\n",
    "        hh_concat = np.sort(datacut.flatten())\n",
    "    else: \n",
    "        for i in coords_list[count]:        \n",
    "            #unpacking coords\n",
    "            position = i[0]\n",
    "            size = i[1]\n",
    "\n",
    "            #cutting out coordinates\n",
    "            cutout = Cutout2D(data, position, size, wcs = w)\n",
    "            datacut = cutout.data\n",
    "            wcscut = cutout.wcs\n",
    "\n",
    "            #     flatting data and plotting EDF\n",
    "            flat_data = np.sort(datacut.flatten())\n",
    "            hh_concat = np.concatenate((hh_concat, flat_data), axis=None)\n",
    "    \n",
    "    #flattened and sorted data can now be turned into EDFs!\n",
    "    xedf, yedf = edf_calc(hh_concat)\n",
    "\n",
    "#     plt.figure(111)\n",
    "#     plt.scatter(xedf[a0:af], yedf[a0:af], label = file.split(\"_\")[2], s = 0.1)\n",
    "#     plt.scatter(xedf[int(1e2):], yedf[int(1e2):], label = file.split(\"_\")[2], s = 0.1)\n",
    "#     print(file, len(xedf), len(yedf))\n",
    "#     ax1.scatter(xedf, yedf, label = file.split(\"_\")[2].replace('hh', 'hh ').upper(), s=0.1)\n",
    "    ax1.plot(xedf, yedf, label = file.split(\"_\")[2].replace('hh', 'hh ').upper(), linestyle='--')\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "#now doing amanda's knots (the souther outflows)\n",
    "filenames = glob.glob('hh_div/*amanda*.fits')\n",
    "filenames = ['hh_div_moreratios/amanda_126to164ratio_knot1.fits', 'hh_div_moreratios/amanda_126to164ratio_knot2.fits', \n",
    "             'hh_div_moreratios/amanda_126to164ratio_knot4.fits', 'hh_div_moreratios/amanda_126to164ratio_knot3.fits', \n",
    "             'hh_div_moreratios/amanda_126to164ratio_knot5.fits', 'hh_div_moreratios/amanda_126to164ratio_knot6.fits']\n",
    "\n",
    "coords_list =  [\n",
    "#                 [[(30, 72), (20, 30)], [(105, 42), (50, 50)]],\n",
    "                [[(30, 71), (20, 16)], [(105, 49), (24, 27)], [(119, 26), (15, 12)]],\n",
    "#                 [[(40, 66), (20, 20)], [(61, 30), (22, 45)]],\n",
    "                [[(38, 67), (24, 11)], [(70, 32), (23, 20)]],\n",
    "                [[(64, 55), (33, 49)]],            \n",
    "                [[(118, 91), (48, 55)], [(232, 50), (48, 39)]],\n",
    "                [[(27, 21), (19, 19)], [(83, 70), (46, 65)],[(118, 85), (21, 91)], [(148, 120), (33, 56)]],\n",
    "                [[(78, 161), (49, 51)], [(100, 95), (43, 72)], [(139, 50), (44, 25)]]\n",
    "                    ]\n",
    "\n",
    "\n",
    "names_list = ['SVS 13B - HH A', 'SVS 13B - HH B', 'HH 344', 'SVS 13B - HH C', 'SVS 13B - HH D', 'SVS 13B - HH E1']\n",
    "\n",
    "count = 0\n",
    "for file in filenames:    \n",
    "    w, data = file_open(file)  \n",
    "    #plotting\n",
    "#     implot(data, w, False, 3) \n",
    "    \n",
    "    hh_concat = [] #initializing list to join all the hh6 knots\n",
    "    if count > 1:\n",
    "        for i in coords_list[count]:        \n",
    "            #unpacking coords\n",
    "            position = i[0]\n",
    "            size = i[1]\n",
    "\n",
    "            #cutting out coordinates\n",
    "            cutout = Cutout2D(data, position, size, wcs = w)\n",
    "            datacut = cutout.data\n",
    "            wcscut = cutout.wcs\n",
    "\n",
    "            #     flatting data and plotting EDF\n",
    "            flat_data = np.sort(datacut.flatten())\n",
    "            hh_concat = np.concatenate((hh_concat, flat_data), axis=None)\n",
    "\n",
    "        #flattened and sorted data can now be turned into EDFs!\n",
    "        xedf, yedf = edf_calc(hh_concat)\n",
    "#         ax1.scatter(xedf, yedf, label = 'SVS 13B - HH '+file.split(\"_\")[-1][4:-5], s=0.1)\n",
    "        ax1.plot(xedf, yedf, label = names_list[count], linestyle='--')\n",
    "    count += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(111)\n",
    "# ax1.plot([2.6, 2.6], [0,1.1], label ='x=2.6 (This Work)', linestyle=':')\n",
    "# ax1.plot([1.25, 1.25], [0,1.1], label ='x=1.25 (Bautista+2015)', linestyle=':')\n",
    "# ax1.plot([2.39, 2.39], [0,1.1], label ='x=2.39 Quinet96corr', linestyle=':')\n",
    "# ax1.plot([3.85, 3.85], [0,1.1], label ='x=3.85 (Nahar95, CHIANTI)', linestyle=':')\n",
    "# ax1.set_ylim(0.0, 1.01)\n",
    "# ax1.set_xlim(0, 4)\n",
    "# plt.ylim(0.8, 1.01) #alt lims\n",
    "# plt.xlim(0.5, 4)\n",
    "# ax1.set_title('EDFs')\n",
    "# ax1.set_xlabel('x [1.26/1.64 Flux]')\n",
    "ax1.set_ylabel('Cumulative Probability')\n",
    "\n",
    "#need to reorder legend *because of literally one thing*\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "ind_reorder = labels.index('HH 5')\n",
    "handles.insert(0, handles[ind_reorder])\n",
    "labels.insert(0, labels[ind_reorder])\n",
    "handles.pop(labels.index('HH 5', 1))\n",
    "labels.pop(labels.index('HH 5', 1))\n",
    "ax1.legend(handles, labels, loc='best', markerscale = 20, ncol=2)\n",
    "\n",
    "# plt.savefig('EDFs/EDFMAKER_otherEDF.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#next part: EDF fitting / histograms'\n",
    "#establishing reference lines from literature\n",
    "ax2.plot([2.6, 2.6], [0,1e3], label ='This Work (2.6)', color='black', linestyle='-', linewidth=3)\n",
    "ax2.plot([0.69, 0.69], [0,1e3], color='black', linestyle=':') \n",
    "ax2.plot([1.03, 1.03], [0,1e3], color='black', linestyle=':') \n",
    "ax2.plot([1.04, 1.04], [0,1e3], color='black', linestyle=':') \n",
    "ax2.plot([1.17, 1.17], [0,1e3], color='black', linestyle=':') \n",
    "# ax2.plot([1.25, 1.25], [0,1e3], label ='Recommended Literature Values', color='black', linestyle=':', linewidth=3) #bautista (2015) rec\n",
    "ax2.plot([1.25, 1.25], [0,1e3], color='black', linestyle=':') #bautista (2015) rec\n",
    "ax2.plot([1.18, 1.18], [0,1e3], label ='Literature Values', color='black', linestyle=':')\n",
    "ax2.plot([1.26, 1.26], [0,1e3], color='black', linestyle=':') \n",
    "ax2.plot([1.32, 1.32], [0,1e3], color='black', linestyle=':') #bautista et al 2017\n",
    "# ax2.plot([1.36, 1.36], [0,1e3], color='black', linestyle=':', linewidth=3) #debb hibbert rec & Nussbaumer storey 1988\n",
    "ax2.plot([1.36, 1.36], [0,1e3], color='black', linestyle=':') #debb hibbert rec & Nussbaumer storey 1988\n",
    "ax2.plot([1.50, 1.50], [0,1e3], color='black', linestyle=':') \n",
    "ax2.plot([1.96, 1.96], [0, 1e3], color='black', linestyle=':') #Tayal & Zatsarinni 2018\n",
    "ax2.plot([2.39, 2.39], [0,1e3], color='black', linestyle=':') \n",
    "ax2.plot([3.8, 3.8], [0,1e3], color='black', linestyle=':')\n",
    "# ax2.plot([1.18, 1.18], [0,1e3], label ='1.18 Quinet et al. 1996 HFR', color='black', linestyle='--')\n",
    "# ax2.plot([1.25, 1.25], [0,1e3], label ='1.25 Bautista et al. 2015', color='black', linestyle= (0, (5, 10))) #looseley dashed\n",
    "# ax2.plot([1.36, 1.36], [0,1e3], label ='1.36 Debb & Hibbert 2011', color='black', linestyle=':')\n",
    "# ax2.plot([2.39, 2.39], [0,1e3], label ='2.39 Bautista et al. 2015 & Quinet et al. 1996', color='black', linestyle=(0, (1, 10))) #looseley dotted\n",
    "# ax2.plot([3.85, 3.85], [0,1e3], label ='3.85 Nahar 1995', color='black', linestyle='-.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "perc=0.2\n",
    "file = 'hh_div/regrid_hh6_ston_'+str(perc)+'.fits'\n",
    "w, data = file_open(file)\n",
    "coords_list = [[(53, 64), (38, 35)], [(95, 72), (17, 29)],[(103, 116), (35, 34)],[(166, 111), (38, 38)],[(175, 163), (30, 29)],[(223, 165), (34, 37)],]\n",
    "hh6_concat = [] #initializing list to join all the hh6 knots\n",
    "\n",
    "for i in range(len(coords_list)):\n",
    "    #unpacking coords\n",
    "    position = coords_list[i][0]\n",
    "    size = coords_list[i][1]\n",
    "\n",
    "    #cutting out coordinates\n",
    "    cutout = Cutout2D(data, position, size, wcs = w)\n",
    "    datacut = cutout.data\n",
    "    wcscut = cutout.wcs\n",
    "\n",
    "\n",
    "    #     flatting data\n",
    "    flat_data = np.sort(datacut.flatten())\n",
    "    \n",
    "    #incase needing to restrict data...\n",
    "    hh6_concat = np.concatenate((hh6_concat, flat_data), axis=None)\n",
    "\n",
    "\n",
    "xedf, yedf = edf_calc(hh6_concat)\n",
    "a0 = int(0)\n",
    "af = int(6575)\n",
    "\n",
    "# plt.figure(113)\n",
    "n, bins, patches = ax2.hist(hh6_concat, label = 'HH 6', density=False, histtype ='step', bins=50, linewidth=2, color='green')  # density=False would make counts\n",
    "print('hh6, 12 bins, N/sqrt(N)', n[-15:] / np.sqrt(n[-15:]), 'bin loc', bins[-15:])\n",
    "print(bins[0:-13], len(bins), len(n))\n",
    "print('fraction of total counts for hh6 @ 2.6', np.sum(n[0:-13]) / np.sum(n))\n",
    "\n",
    "\n",
    "#reading in data\n",
    "perc = 0.2\n",
    "file = 'hh_div/regrid_hh711_ston_'+str(perc)+'.fits'\n",
    "w, data = file_open(file)\n",
    "               \n",
    "coords_list = [[(583, 441), (36, 39)]]\n",
    "\n",
    "for i in range(len(coords_list)):\n",
    "    #unpacking coords\n",
    "    position = coords_list[i][0]\n",
    "    size = coords_list[i][1]\n",
    "\n",
    "    #cutting out coordinates\n",
    "    cutout = Cutout2D(data, position, size, wcs = w)\n",
    "    datacut = cutout.data\n",
    "    wcscut = cutout.wcs\n",
    "\n",
    "    #     flattening data\n",
    "    flat_data = datacut.flatten()\n",
    "    xedf, yedf = edf_calc(flat_data)\n",
    "\n",
    "    #plotting only a limited range of data to check\n",
    "#     plt.figure(112)\n",
    "#     plt.scatter(xedf, yedf, label =  'hh'+str(i+10), s = 0.1)\n",
    "    \n",
    "#     plt.figure(113)\n",
    "    n, bins, patches = ax2.hist(flat_data, label = 'HH '+str(i+10), histtype ='step', density=False, bins=50, linewidth=2, color='blue')  # density=False would make counts\n",
    "    print('hh10, 60 bins, N/sqrt(N)', n[-7:] / np.sqrt(n[-7:]), 'bin loc', bins[-7:])\n",
    "    print(bins[0:-4], len(bins), len(n))\n",
    "    print('fraction of total counts for hh 10 at 2.6', np.sum(n[0:-4]) / np.sum(n))\n",
    "\n",
    "    \n",
    "    a0 = int(0)\n",
    "    af = -1\n",
    "#     popt, deriv2 = func_fitter(xedf[a0:af], yedf[a0:af], 'hh'+str(i+10), 5000, p0=[1,1,1,1,1,1], pdf_scale=np.max(n))\n",
    "#     zero_finder(deriv2, xedf[a0:af])\n",
    "    \n",
    "#     kn = KneeLocator(xedf[0:af], edf_func(xedf[0:af], *popt), curve='concave', direction='increasing')\n",
    "#     print('hh'+str(i+10)+' knee: ', kn.knee)\n",
    "    \n",
    "\n",
    "\n",
    "# popt, deriv2 = func_fitter(xedf[a0:af], yedf[a0:af], 'hh6', 5000, p0=[1,1,1,1,1,1], pdf_scale=np.max(n))\n",
    "# zero_finder(deriv2, xedf[a0:af])\n",
    "\n",
    "# kn = KneeLocator(xedf[5500:af], edf_func(xedf[5500:af], *popt), curve='concave', direction='increasing')\n",
    "# print('hh 6 knee: ', kn.knee)\n",
    "\n",
    "# plt.figure(112)\n",
    "# plt.title('EDF+fit')\n",
    "# # plt.scatter(xedf, yedf, label = 'hh6', s = 0.1)\n",
    "# plt.xlabel('1.26/1.64 Flux')\n",
    "# plt.ylabel('Cumulative Probability')\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "# plt.figure(113)\n",
    "# ax2.set_title('first deriv (histogram)')\n",
    "ax2.set_ylim(1, 1e3)\n",
    "ax2.set_xlim(0, 4)\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_ylabel('Number of Pixels')\n",
    "ax2.set_xlabel(r'$\\rm [1.26 \\, \\mu m \\, / \\, 1.64 \\, \\mu m]$ Intensity')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# plt.savefig('Rubinstein_NGC1333_FeII_highres.png', dpi=500)\n",
    "\n",
    "\n",
    "# fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
