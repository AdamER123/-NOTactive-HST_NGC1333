{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#check what version of python you're using - I'm using 3.7.3\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#importing libraries\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "from astropy.wcs import WCS\n",
    "from reproject import reproject_exact  #a package that can be added to astropy using anaconda or pip (see their docs pg)\n",
    "from reproject import reproject_interp\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #finding the path to every fits images in a directory\n",
    "def im_name_finder(path, file_type):\n",
    "    #Using glob (it's a unix command similar to ls)\n",
    "    #WARNING: using recursive=True...depending how many images you use this could be very slow, it's recommended not to have too many subfolders\n",
    "    #if needed, some example code is commented towards the latter half of this code that could help make an alternative\n",
    "    all_names = glob.glob(path, recursive=True)\n",
    "\n",
    "    #IMPORTANT: Using \"fit\" here because it is inclusive of both fits and FIT...some files end in \"FIT\" and need to be included\n",
    "    #using s.lower() include uppercase names\n",
    "    im_names = [s for s in all_names if 'fit' in s.lower()]\n",
    "\n",
    "    return im_names\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "#setting up a new fits file to be saved and viewed in DS9\n",
    "#primarily to save the image we reprojected, but can also be used to save the convolved images\n",
    "def fits_saver(array, wcs_header, name, save_path):\n",
    "    '''\n",
    "    array is a 2d array of data - could be from reprojecting one image onto another or from convolution\n",
    "    wcs_header is a header containing the wcs coords of the image that we projected onto or of the orig image (if from the convolution)\n",
    "    name is the path to some image you're using. It will get string split at the / character, and the func only takes the last element of that splitting\n",
    "    save_path is the folder you want to save to...recommended to also add something to the start of the images names to make it clear what you did to them (e.g. 'Regridded/regrid_')\n",
    "    '''\n",
    "\n",
    "    #creating a new file and adding the reprojected array of data as well as the WCS that we projected onto\n",
    "    hdu_new = fits.PrimaryHDU(array, header=wcs_header)\n",
    "    hdul = fits.HDUList([hdu_new])\n",
    "\n",
    "    #saving the file\n",
    "    new_filename = name.split('/')[-1]  #grabs the file name we were using from before\n",
    "    hdul.writeto(save_path+new_filename, overwrite=True)\n",
    "\n",
    "    return (save_path+new_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[21]:\n",
    "#declaring noise: measured from the RMS of a region of empty sky while opening the image in DS9\n",
    "#these are newly taken from the HH 6 images...\n",
    "# noise_126 = 0.0465246\n",
    "# noise_128 = 0.04733\n",
    "# noise_164 = 0.0561296\n",
    "\n",
    "#from the regridded images...\n",
    "noise_126 = 0.216037\n",
    "noise_128 = 0.258513\n",
    "noise_164 = 0.39321\n",
    "noise_halpha = 0.302712\n",
    "#converting units for reference\n",
    "#hdu1_conv_scaled = hdu1_conv_scaled * hdu1_fnu / 1e6 #converting to MJy\n",
    "#hdu1_conv_scaled = hdu1_conv_scaled / hdu1_pix**2. * 4.25e10 #dividing out sr, D001SCAL is key for pixel size in arcsec\n",
    "\n",
    "\n",
    "\n",
    "#EX: grabbing all the fits image paths in a directory, so they can be looped through and their data opened\n",
    "#set your path to some directory with images (the images can be in subdirectories)\n",
    "#using ** will grab all files even in subdirectories...WARNING this will take longer\n",
    "path = '../../../ngc1333_fits/'\n",
    "im_names_hub_dash = im_name_finder(path+'*', 'fit')\n",
    "im_names_hub_dash = [i.replace('\\\\', '/') for i in im_names_hub_dash]\n",
    "im_names_hub = [path+'126_image.fits', path+'128_image.fits', path+'164_image.fits']\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "#Minimal loop through all the images, including a try/except in case an image is faulty for whatever reason\n",
    "#IMPORTANT: A more detailed example between two single images is at the end of this code with many more comments\n",
    "\n",
    "#First, we need to setup the image we're projecting ONTO\n",
    "#This will be the same no matter the loop, so only need to do this once\n",
    "#low_res = [x for x in im_names_spitz if 'n1333_lh_3_SiII_' in x][0]  #finding the lowest res image - LH and long lambda, so [SiII]\n",
    "#hdu1 = fits.open(low_res)[0]\n",
    "low_res = im_names_hub_dash[0]   #Could also let this be a hubble image...if so, see the hubble loop below for how to setup grabbing the data, header, pixel conversion from the hdu\n",
    "hdu1 = fits.open(low_res)\n",
    "\n",
    "#hdu1_pix = hdu1.header['CDELT2'] #the pixel size in degrees, CDELT is the keyword for Spitzer images\n",
    "#hdu1_pix_torad = hdu1_pix * np.pi / 180.\n",
    "hdu1_pix = hdu1[0].header['D001SCAL'] #same as above line, but D001SCAL is the keyword for Hubble images\n",
    "hdu1_pix_torad = hdu1_pix / 206265.\n",
    "hdu1_fnu = hdu1[0].header['PHOTFNU']\n",
    "\n",
    "low_res = im_names_hub[0]\n",
    "hdu1 = fits.open(low_res)\n",
    "hdu1_data = hdu1[0].data\n",
    "hdu1_header = hdu1[0].header\n",
    "\n",
    "#converting noise units\n",
    "noise_126 = noise_126 * hdu1_fnu / 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam2\\Anaconda\\envs\\mypython3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "for name in [im_names_hub[1]]:\n",
    "    hdu2 = fits.open(name)  #Image we're reprojecting...no index yet since some useful data split between headers\n",
    "    hdu2 = fits.open(im_names_hub_dash[2])\n",
    "    #try:\n",
    "\n",
    "    #reading in data\n",
    "    hdu2_pix = hdu2[0].header['D001SCAL'] #same as above line, but D001SCAL is the keyword for Hubble images\n",
    "    hdu2_pix_torad = hdu2_pix / 206265.\n",
    "    hdu2_fnu = hdu2[0].header['PHOTFNU']\n",
    "\n",
    "    hdu2 = fits.open(name)\n",
    "    hdu2_data = hdu2[0].data\n",
    "    hdu2_header = hdu2[0].header\n",
    "\n",
    "    #converting noise units\n",
    "    noise_128 = noise_128 * hdu2_fnu / 1e6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #need to do 164 image as well...\n",
    "    name2 = im_names_hub[2]\n",
    "    hdu3 = fits.open(im_names_hub_dash[4])\n",
    "    #try:\n",
    "\n",
    "    #reading in data\n",
    "    hdu3_pix = hdu3[0].header['D001SCAL'] #same as above line, but D001SCAL is the keyword for Hubble images\n",
    "    hdu3_pix_torad = hdu3_pix / 206265.\n",
    "    hdu3_fnu = hdu3[0].header['PHOTFNU']\n",
    "\n",
    "    hdu3 = fits.open(name2)\n",
    "    hdu3_data = hdu3[0].data\n",
    "    hdu3_header = hdu3[0].header\n",
    "\n",
    "    #converting noise units\n",
    "    noise_164 = noise_164 * hdu3_fnu / 1e6\n",
    "\n",
    "\n",
    "\n",
    "    #reprojection of one hdu using the header (coords and pixels) of another\n",
    "    #The first input is the path to the file we're reprojecting. The second input is the header of the image we're projecting ONTO\n",
    "    #you'll need to set the WCS to be that of the header you're basing this off of...ie the header\n",
    "\n",
    "    #file_start = 'Convolved_Images_Hub/conv_'\n",
    "    #conv2_path = file_start + name.split('/')[-1]\n",
    "\n",
    "    w = WCS(hdu1_header)\n",
    "    wcs_header = w.to_header()\n",
    "    file_start = '../../Convolved_Images_Hub/conv_'\n",
    "    hdu1_conv = fits.open(file_start + low_res.split('/')[-1])\n",
    "    hdu1_conv_scaled = hdu1_conv[0].data * hdu1_pix**2. / 4.25e10\n",
    "\n",
    "\n",
    "    #para is False for large images (like these hubble ones)\n",
    "    #output is array (a 2D array of data) and footprint (the footprint from the analysis)\n",
    "    file_start = '../../Regridded_Hub/regrid_'\n",
    "    hdu_regrid_hub = fits.open(file_start + name.split('/')[-1])\n",
    "    array = hdu_regrid_hub[0].data * hdu2_pix**2. / 4.25e10\n",
    "\n",
    "\n",
    "    hdu_regrid_hub2 = fits.open(file_start + name2.split('/')[-1])\n",
    "    array2 = hdu_regrid_hub2[0].data * hdu3_pix**2. / 4.25e10\n",
    "\n",
    "\n",
    "    #now that we have a reprojected hubble image for hdu2 and both are convolved, need to\n",
    "    #divide one by the other...then can use the same wcs header that we projected onto (hdu1's)!\n",
    "    #getting rid of nan values\n",
    "    where_are_NaNs = np.isnan(array)\n",
    "    array[where_are_NaNs] = 0.\n",
    "\n",
    "    where_are_NaNs = np.isnan(array2)\n",
    "    array2[where_are_NaNs] = 0.\n",
    "\n",
    "    where_are_NaNs = np.isnan(hdu1_conv_scaled)\n",
    "    hdu1_conv_scaled[where_are_NaNs] = 0.\n",
    "\n",
    "    #print(perc * np.max(array))\n",
    "    #array[array < (perc * np.max(array))] = 0.  #only taking values greater than some percent of the maximum\n",
    "\n",
    "\n",
    "    #scaling by the signal to noise\n",
    "    perc = 0.8 #this tends to work?? Maybe can change...\n",
    "    \n",
    "    array[array < noise_128*perc] = 0.\n",
    "    array2[array2 < noise_164*perc] = 0.\n",
    "    hdu1_conv_scaled[hdu1_conv_scaled < noise_126*perc] = 0.\n",
    "\n",
    "    #getting rid of 0 values\n",
    "    array[array < 0.] = 0.\n",
    "    array2[array2 < 0.] = 0.\n",
    "    hdu1_conv_scaled[hdu1_conv_scaled < 0.] = 0.\n",
    "\n",
    "\n",
    "\n",
    "#     eps = 8e-1 \n",
    "#     epslist = [1e-3, 1e-2, 5e-2, 1e-1, 5e-1, 7e-1, 8e-1, 9e-1, 99e-1]\n",
    "    epslist = [0.0]\n",
    "    \n",
    "    for eps in epslist:\n",
    "        flux01 = hdu1_conv_scaled - eps * array\n",
    "        flux02 = array2\n",
    "        data_ratio = np.divide(flux01, flux02, out=np.zeros_like(flux02), where=flux02!=0.) #need to do np.divide to guarantee we get no divide by zero issue...\n",
    "\n",
    "\n",
    "        # remember to have the right header with the wcs below and that it matches the one we're projecting ONTO\n",
    "        w = WCS(hdu1_header)\n",
    "        wcs_header = w.to_header()\n",
    "        save_path = './regrid_'  #See fits_saver's \"save_path\" description for explanation\n",
    "        fits_saver(data_ratio, wcs_header, 'hub_dash_noleakage_ston_'+str(perc)+'.fits', save_path)  #saving the reprojected image\n",
    "\n",
    "    \n",
    "\n",
    "sys.exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./nonlintest_hub_dash_noleakage_ston_-1.fits'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'inanna_runs/'\n",
    "im_names_hub = [path+'nonlin_flam_1_26_mic.fits', path+'nonlin_flam_1_64_mic.fits']\n",
    "\n",
    "im_126 = im_names_hub[0]   \n",
    "hdu1 = fits.open(im_126)\n",
    "hdu1_data = hdu1[0].data\n",
    "hdu1_header = hdu1[0].header\n",
    "\n",
    "im_164 = im_names_hub[1]   \n",
    "hdu2 = fits.open(im_164)\n",
    "hdu2_data = hdu2[0].data\n",
    "hdu2_header = hdu2[0].header\n",
    "\n",
    "\n",
    "data_ratio = np.divide(hdu1_data, hdu2_data, out=np.zeros_like(hdu2_data), where=hdu2_data!=0.) #need to do np.divide to guarantee we get no divide by zero issue...\n",
    "\n",
    "\n",
    "# remember to have the right header with the wcs below and that it matches the one we're projecting ONTO\n",
    "w = WCS(hdu1_header)\n",
    "wcs_header = w.to_header()\n",
    "save_path = './nonlintest_'  #See fits_saver's \"save_path\" description for explanation\n",
    "fits_saver(data_ratio, wcs_header, 'hub_dash_noleakage_ston_-1.fits', save_path)  #saving the reprojected image\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
