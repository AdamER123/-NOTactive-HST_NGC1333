{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arubi\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data!\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#importing libraries\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve, Gaussian2DKernel, Box2DKernel\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.wcs import WCS\n",
    "# from reproject import reproject_exact  #a package that can be added to astropy using anaconda or pip (see their docs pg)\n",
    "# from reproject import reproject_interp\n",
    "\n",
    "import glob\n",
    "import matplotlib \n",
    "matplotlib.use('Agg') #invokved b/c just plain matplotlib was insufficient\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "#switches for the three different parts of this code\n",
    "switch1 = 'on' #convolving images [needed to put it on for switch 3 at min...need to figure out other solution, eh]\n",
    "switch1b = 'on' #regridding...\n",
    "switch2 = 'on' #solving equations\n",
    "switch3 = 'on' #plotting / graphics of solutions\n",
    "\n",
    "\n",
    "if switch1 == 'on':\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "    # #finding the path to every fits images in a directory\n",
    "    def im_name_finder(path, file_type):\n",
    "        #Using glob (it's a unix command similar to ls)\n",
    "        #WARNING: using recursive=True...depending how many images you use this could be very slow, it's recommended not to have too many subfolders\n",
    "        #if needed, some example code is commented towards the latter half of this code that could help make an alternative\n",
    "        all_names = glob.glob(path, recursive=True)\n",
    "\n",
    "        #IMPORTANT: Using \"fit\" here because it is inclusive of both fits and FIT...some files end in \"FIT\" and need to be included\n",
    "        #using s.lower() include uppercase names\n",
    "        im_names = [s for s in all_names if 'fit' in s.lower()]\n",
    "\n",
    "        return im_names\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "    '''now convolve my image with a PSF of the image we're projecting ONTO\n",
    "    an approx PSF can be found by assuming a 2D Gaussian func with a width (a FWHM) of the diffrac limit\n",
    "    that is the st dev of the Gaussian is about the st dev is about = lambda/D\n",
    "    a list of PSFs are found on https://docs.astropy.org/en/stable/convolution/kernels.html\n",
    "\n",
    "    Notes:\n",
    "    FIRST: always must convert hdu1_pixtorad to radians! It's inconsistent otherwise, and lambda/D is generally in radians\n",
    "\n",
    "    what we're using for the gaussian width is the FWHM, not the radius of the first ring of the diffraction pattern,\n",
    "    so it's 1.2 not 1.22 times lambda/D\n",
    "\n",
    "    D is 85 cm for spitzer\n",
    "    D is 2.4 m for hubble\n",
    "    '''\n",
    "\n",
    "    def im_conv(low_res_name, D, hdu_pix_torad, hdu_dat, kern, fwhm=None):\n",
    "        #unfortuantely no good way to find wavelength from header right now. can enter it manually, but I tried to automate it\n",
    "\n",
    "        #reading in excel file of wavelengths...right now needs to be in same directory as this code\n",
    "        #first col is a substring of the fits image file name, the second col is the wavelengths in microns\n",
    "        df = pd.read_excel('../../../imglams.xlsx')\n",
    "        cols = df.columns\n",
    "        cols_str = [str(i) for i in df[cols[0]]]\n",
    "        #some test cases I was using\n",
    "        \n",
    "        if kern == 'epsf_fwhm':\n",
    "            kernel = Gaussian2DKernel(fwhm, y_stddev=fwhm)\n",
    "        \n",
    "        #gaussian kernel\n",
    "        if kern == 'gauss':\n",
    "            #this finds the loc in the excel file where the image substring matches our image name\n",
    "            #it then finds the wavelength value corresponding to that loc\n",
    "            lam =  df.loc[np.where([i in low_res_name for i in cols_str])[0][0]].values[1] #lambda in microns\n",
    "            \n",
    "            #finding angular resolution...the FWHM of our Gaussian PSF\n",
    "            res = 1.2 * lam / D         #resolution in radians\n",
    "#             print('Angular Res (rads): ', res, ' File Name to Convolve With: ', low_res_name, ' Lambda: ', lam)\n",
    "            res = res / hdu_pix_torad        #so converting to pixels\n",
    "#             print('Angular Res (pixels): ', res, ' File Name to Convolve With: ', low_res_name, ' Lambda: ', lam)\n",
    "\n",
    "            #finding PSF and then calculating the convolution of our image and the PSF of the image we're projecting onto\n",
    "            kernel = Gaussian2DKernel(res)\n",
    "        \n",
    "        #box kernel\n",
    "        if kern == 'box':\n",
    "            kernel = Box2DKernel(16.)\n",
    "\n",
    "        hdu_conv = convolve(hdu_dat, kernel)\n",
    "        return hdu_conv\n",
    "\n",
    "\n",
    "    # In[21]:\n",
    "    \n",
    "    \n",
    "    #setting up a new fits file to be saved and viewed in DS9\n",
    "    #primarily to save the image we reprojected, but can also be used to save the convolved images\n",
    "    def fits_saver(array, wcs_header, name, save_path):\n",
    "        '''\n",
    "        array is a 2d array of data - could be from reprojecting one image onto another or from convolution\n",
    "        wcs_header is a header containing the wcs coords of the image that we projected onto or of the orig image (if from the convolution)\n",
    "        name is the path to some image you're using. It will get string split at the / character, and the func only takes the last element of that splitting\n",
    "        save_path is the folder you want to save to...recommended to also add something to the start of the images names to make it clear what you did to them (e.g. 'Regridded/regrid_')\n",
    "        '''\n",
    "\n",
    "        #creating a new file and adding the reprojected array of data as well as the WCS that we projected onto\n",
    "        hdu_new = fits.PrimaryHDU(array, header=wcs_header)\n",
    "        hdul = fits.HDUList([hdu_new])\n",
    "\n",
    "        #saving the file\n",
    "        new_filename = name.split('/')[-1]  #grabs the file name we were using from before\n",
    "        hdul.writeto(save_path+new_filename, overwrite=True)\n",
    "\n",
    "        return (save_path+new_filename)\n",
    "    \n",
    "    \n",
    "\n",
    "    #EX: grabbing all the fits image paths in a directory, so they can be looped through and their data opened\n",
    "    #set your path to some directory with images (the images can be in subdirectories)\n",
    "    #using ** will grab all files even in subdirectories...WARNING this will take longer\n",
    "\n",
    "    # In[28]:\n",
    "    #this time setting up the file names by hand since I've found that easier...\n",
    "    #order: halpha or .656 mic, 0.672 mic, 1.26, 1.28, 1.64\n",
    "    files_units = ['../../../../ngc1333_fits/unregridded/656_image.fits', \n",
    "                   '../../../../ngc1333_fits/unregridded/0301_flt.fits', \n",
    "                   '../../../../ngc1333_fits/unregridded/0501_flt.fits', \n",
    "                   '../../../../ngc1333_fits/126build_shift_2_drz.fits', \n",
    "                   '../../../../ngc1333_fits/128build_shift_2_drz.fits', \n",
    "                   '../../../../ngc1333_fits/164build_shift_2_drz.fits']\n",
    "    hdu_list_units = [fits.open(i) for i in files_units]\n",
    "    files_data = ['../../../../ngc1333_fits/0301_oIreproject2.fits', \n",
    "                  '../../../../ngc1333_fits/656_hareproject_shifted_up5_left3.fits', \n",
    "                  '../../../../ngc1333_fits/672_sIIreproject.fits', \n",
    "                  '../../../../ngc1333_fits/Background_corr/background_corr_126_aligned.fits', \n",
    "                  '../../../../ngc1333_fits/Background_corr/background_corr_128_aligned.fits', \n",
    "                  '../../../../ngc1333_fits/Background_corr/background_corr_164_aligned.fits']\n",
    "    hdu_list = [fits.open(i) for i in files_data]\n",
    "\n",
    "    hdu_pix_list = []\n",
    "    hdu_pixtorad_list = []\n",
    "    hdu_fnu_list = []\n",
    "    hdu_flam_list = []\n",
    "    hdu_bw_list = []\n",
    "    hdu_data_list = []\n",
    "    hdu_header_list = []\n",
    "    throughput_list = [1., 1., 1., 1., 1., 1.] # [0.242, 1., 0.246, 0.496, 0.521, 0.470] #also has to be done by hand, not in the headers?\n",
    "\n",
    "\n",
    "    #I'm using count here just to point to specific indices that I've set up...unfortunately some have different headers...\n",
    "    #the only diff between the if and else cases are the indexing of the hdu's, some need 1 and some need 0\n",
    "    #I've tried to group it for convience, so the the first two have the same headers, the last 3 have the same headers\n",
    "    count = 0\n",
    "    for (hdu_units,hdu_data) in zip(hdu_list_units, hdu_list):\n",
    "        if count <= 2: # == 0 or count == 2:\n",
    "            #reading in conversions\n",
    "            hdu_pix_list.append(hdu_units[0].header['D001SCAL'])  #D001SCAL is the keyword for Hubble images\n",
    "            hdu_pixtorad_list.append(hdu_pix_list[count] / 206265.)\n",
    "#             print('D001SCAL: ', hdu_pix_list[count], ' D001SCAL/206265: ', hdu_pixtorad_list[count])\n",
    "            # hdu_fnu_list.append(hdu_units[1].header['PHOTFNU'])\n",
    "            hdu_flam_list.append(hdu_units[1].header['PHOTFLAM'])\n",
    "            hdu_bw_list.append(hdu_units[1].header['PHOTBW'])\n",
    "\n",
    "            #reading in datafor general use  and header for wcs\n",
    "            hdu_data_list.append(hdu_data[0].data)\n",
    "            hdu_header_list.append(hdu_data[0].header)\n",
    "\n",
    "        else:\n",
    "            #reading in conversions\n",
    "            hdu_pix_list.append(hdu_units[0].header['D001SCAL'])  #D001SCAL is the keyword for Hubble images\n",
    "            hdu_pixtorad_list.append(hdu_pix_list[count] / 206265.)\n",
    "#             print('D001SCAL: ', hdu_pix_list[count], ' D001SCAL/206265: ', hdu_pixtorad_list[count])\n",
    "            # hdu_fnu_list.append(hdu_units[0].header['PHOTFNU'])\n",
    "            hdu_flam_list.append(hdu_units[0].header['PHOTFLAM'])\n",
    "            hdu_bw_list.append(hdu_units[0].header['PHOTBW'])\n",
    "\n",
    "            #reading in datafor general use  and header for wcs\n",
    "            hdu_data_list.append(hdu_data[0].data)\n",
    "            hdu_header_list.append(hdu_data[0].header)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    #can update later...but basically the sulfur II image header isn't avail...\n",
    "    #header info taken from https://www.stsci.edu/hst/instrumentation/wfc3/data-analysis/photometric-calibration/uvis-photometric-calibration/quad-filter-photometry\n",
    "    #update: HAlpha etc uses https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/wfc3/documentation/instrument-science-reports-isrs/_documents/2021/WFC3_ISR_2021-04.pdf\n",
    "    hdu_flam_list[0] = 1.6600e-17\n",
    "    hdu_bw_list[0] = 41.77 #from Dan, in A\n",
    "    hdu_flam_list[2] = 1.3699e-17\n",
    "    hdu_bw_list[2] = 18.5\n",
    "\n",
    "    print('loaded data!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_str = '656shifted_flam' #used to label what we're saving, usually related to units or whether we're doing a gaussian or box convolution, etc\n",
    "# resize = 60. #if trying to adjust size of gaussian convolution\n",
    "D = 2.4 #/ resize #that of Hubble, in m\n",
    "D *= 1e6 #converting to microns since x m / 1 m * 1E6 microns gets microns, the unit of our wavelength file\n",
    "\n",
    "#format: conv(data convolving with, Diameter, pix size convolving with, image data to be convolved, convolution method)\n",
    "#do each with its own psf\n",
    "# max_fwhm = np.max(fwhm_list)\n",
    "# fwhm_avg_list = [np.sqrt(max_fwhm**2. - i**2.) for i in fwhm_list]\n",
    "\n",
    "hdu_conv_list = []\n",
    "\n",
    "# fwhm_avg_list = [1.1, 1.59, 1.45, 0, 0.95, 1.2] #based on dan\n",
    "# fwhm_avg_list = [0.7461, 0.65, 0.59, 0, 0.275, 0.6725] #based on astroimagej\n",
    "# \n",
    "fwhm_avg_list = [1.176801567,1.027463529,1.140036453,0,0.968260447,1.062978383] #based on my gaussian2d formula, called sqrtsq_asr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n",
      "WARNING: nan_treatment='interpolate', however, NaN values detected post convolution. A contiguous region of NaN values, larger than the kernel size, are present in the input array. Increase the kernel size to avoid this. [astropy.convolution.convolve]\n"
     ]
    }
   ],
   "source": [
    "l = 0 \n",
    "hdu_conv_list.append(im_conv(files_data[l], D, hdu_pixtorad_list[l], hdu_data_list[l], 'epsf_fwhm', fwhm=fwhm_avg_list[l]))\n",
    "\n",
    "l = 1\n",
    "hdu_conv_list.append(im_conv(files_data[l], D, hdu_pixtorad_list[l], hdu_data_list[l], 'epsf_fwhm', fwhm=fwhm_avg_list[l]))\n",
    "\n",
    "l = 2\n",
    "hdu_conv_list.append(im_conv(files_data[l], D, hdu_pixtorad_list[l], hdu_data_list[l], 'epsf_fwhm', fwhm=fwhm_avg_list[l]))\n",
    "\n",
    "l = 3\n",
    "hdu_conv_list.append(hdu_data_list[l])\n",
    "\n",
    "l = 4\n",
    "hdu_conv_list.append(im_conv(files_data[l], D, hdu_pixtorad_list[l], hdu_data_list[l], 'epsf_fwhm', fwhm=fwhm_avg_list[l]))\n",
    "\n",
    "l = 5\n",
    "hdu_conv_list.append(im_conv(files_data[l], D, hdu_pixtorad_list[l], hdu_data_list[l], 'epsf_fwhm', fwhm=fwhm_avg_list[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "onto converting units\n",
    "'''\n",
    "#converting the convolved image to correct units and saving it so we can reproject it\n",
    "#conversion needed for hubble case since units are not in terms of surface brightness\n",
    "hdu_conv_scaled_list = []\n",
    "\n",
    "#for each convolved image, we need to convert from e-/s to flambda units which are in erg/s/cm^2/Angstrom...multiply by bw to get rid of angstrom\n",
    "#...also images are divided through by throughput, so multiplying by throughput\n",
    "#and lastly dividing by the arcsec^2 to get the image in surface brightness units, to be used in regridding\n",
    "for count, i in enumerate(hdu_conv_list):\n",
    "\n",
    "    #a condition added since the background corrected images have units corrected?\n",
    "    if  count > 2: #the background subtracted images\n",
    "        hdu_conv_scaled_list.append(i) #/ hdu_pixtorad_list[count]**2. ) #note if doing regridding should also add in , regrid only takes surface brightness\n",
    "    elif count <= 2:\n",
    "        hdu_conv_scaled_list.append(i * hdu_flam_list[count] * hdu_bw_list[count])\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "#need a wcs standard for regridding and plots, fits files...\n",
    "w = WCS(hdu_header_list[-1]) #I picked 0 arbitrarily, it shouldn't really matter\n",
    "# wcs_header = w.to_header()\n",
    "wcs_header = hdu_header_list[-1]\n",
    "wcs_header['HISTORY'] = 'Image was convolved and regridded, units were converted only for 656, 672, and 631 images. Units are all flux in CGS (ergs/s/cm^2).'\t#adding history keyword so we know what we did to this image\n",
    "\n",
    "    #you'll need to set the WCS to be that of the header you're basing this off of...ie the header\n",
    "file_start = 'conv_checks/conv_byhand_'+res_str+'_'\n",
    "conv_path_list = [] #list of paths to the convolved images, can be useful...\n",
    "\n",
    "\n",
    "for count, i in enumerate(hdu_header_list):\n",
    "    #finding wcs for a given image\n",
    "    w = WCS(i)\n",
    "    wcs_header = w.to_header()\n",
    "\n",
    "    #saving each file to some path, conv_path is the path to that file\n",
    "    conv_path = fits_saver(hdu_conv_scaled_list[count], wcs_header, files_data[count], file_start)\n",
    "    conv_path_list.append(conv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
